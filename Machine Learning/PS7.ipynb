{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PS7 (CLEAN).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhcnqfawZhmx"
      },
      "source": [
        "# Problem Set 7: Backpropagation\n",
        "# CMSC 422, Fall 2021\n",
        "# Due Nov 18 at 11:59pm\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1914/1*F9capAHwl_rz2-Q8z511WQ.jpeg\" alt=\"meme\" width=\"500px\"/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooh3KTatYqgl"
      },
      "source": [
        "# Instructions\n",
        "In this problem set you will implement backpropagation for a set of different neural network architectures.\n",
        "There is some code provided for you here, and you will write your implementations in the places marked with __```#TODO: Your Code Here```__. You may add helper functions if you feel you need to.\n",
        "\n",
        "__Analysis Questions:__ In addition to Python programming, each problem will contain some analysis questions (under __Analysis__). These are meant to ensure you understand your results, and will be manually graded on Gradescope.\n",
        "\n",
        "__Submission:__ download this notebook as a `.ipynb` file and submit it to Gradescope. This assignment will be partially autograded so follow instructions closely.  \n",
        " \n",
        "- Make sure your plots are visible when downloading the notebook, otherwise they won't appear on Gradescope. \n",
        "- Make sure your code cells are not throwing exceptions.\n",
        "- Please do not import any packages other than what has already been imported here. You may be penalized for doing so.\n",
        "- Lastly, the autograder times out after 40 minutes, so make sure your implementation is relativly efficient (e.g. by using numpy for matrix operations). Our implementation took a little over 10 minutes to test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOtWraGuaWzc"
      },
      "source": [
        "# Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noBuJ-fIZJ0m"
      },
      "source": [
        "## Problem 1 (25 Points)\n",
        "We'll begin with the simplest possible network (a single layer perceptron). It has a single input feature that we call $x$. This is the activation of the single node of the input layer. This is connected to a single output node, which has a weight, $w$. We also have a bias term, so the activation of the output unit is $a = wx + b$. This network will be used to solve a linear regression problem. So, if we are given an input pair of $(x,y)$, we want to minimize the loss: \n",
        "\n",
        "$$L(x,y) = \\frac{1}{1+e^{-(a-y)^2}} - \\frac{1}{2}$$\n",
        "\n",
        "To do this, you will need to randomly initialize the weight and bias and then perform gradient descent.\n",
        "<br>\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1Qz8jJaPXbVzoL44Nd4nbQgFHOA8G_XyI&sz=w1000\" alt=\"net1\" width=\"150px\"/>\n",
        "<br>\n",
        "<i>Figure 1: Network for Problem 1</i>\n",
        "</center>\n",
        "<br>\n",
        "<br>\n",
        "The gradient of the loss is computed using a training set containing pairs, $(x_1, y_1), (x_2, y_2), ... (x_n, y_n)$. We have:\n",
        "\n",
        "$$\\nabla L = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{\\partial L}{\\partial w}(x_i, y_i), \\frac{\\partial L}{\\partial b}(x_i, y_i)  \\right)$$\n",
        "\n",
        "If we denote $\\theta = (w,b)$ as a vector containing all the parameters of the network, we perform gradient\n",
        "descent with the update:\n",
        "\n",
        "$$\\theta^k = \\theta^{k-1} - \\eta \\nabla L$$\n",
        "\n",
        "Here $\\eta$ is the learning rate, and $\\theta^k$ denotes a vector of $(w,b)$ after the $k$'th iteration of gradient descent. Do not mistake $\\eta$ (the learning rate) for $n$ (the number of data points).\n",
        "We provide you with a routine to generate training data. This has the form: \n",
        "\n",
        "```simplest_training_data(n)```  \n",
        "  \n",
        "This just generates $n$ random training points on the line $y = 3x + 2$, with a little Gaussian noise added to the points.  \n",
        "You need to write a routine with the form: \n",
        "\n",
        "```simplest_training(n, k, eta)```\n",
        "\n",
        "Here $n$ indicates the number of points in the training set (you can call `simplest_training_data` to get the training data), $k$ indicates the total number of iterations that you will use in training, and $\\eta$ is the learning rate.  To initialize the weights in your network, we suggest that you initialize $w$ with a Gaussian random variable with mean 0 and variance of 1, and that you initialize $b = 0$.  \n",
        "You also need to write a routine of the form: \n",
        "\n",
        "```simplest_testing(theta, x)```\n",
        "\n",
        "This routine applies the network, using the parameters in theta, to the input values in the vector $x$, and returns a vector of results in $y$.\n",
        "After training, the network should learn $w$ and $b$ values that are similar to those used to train the network.  So you can test your network by looking at the learned $w$ and $b$ values.  Or you can use the testing algorithm to see if the network computes appropriate $y$ values.  In testing, you may find that if you use too big a value for $\\eta$ the network will not converge to anything meaningful.  If you use a value of $k$ that is too small, it won't have time to converge to a good solution.\n",
        "We run our algorithm with $n = 30, k = 10000, \\eta = .02$.   When we test using $x = (0, 1, ..., 9)$ we get the result:\n",
        "  \n",
        "```\n",
        "1.99107688  4.91908171  7.84708654 10.77509137 13.7030962  16.63110103 19.55910586 22.48711069 25.41511552 28.34312035\n",
        "```\n",
        "\n",
        "These points fit the line $y = 3x + 2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZsEHnFFs01Z"
      },
      "source": [
        "import numpy as np\n",
        "import math as m\n",
        "import sys\n",
        "import random\n",
        "from statistics import mean\n",
        "\n",
        "###Problem 1\n",
        "###Provided function to create training data\n",
        "def simplest_training_data(n): # Do NOT edit this function: simplest_training_data(n)!\n",
        "    m = 3\n",
        "    b = 2\n",
        "    x = np.random.uniform(0,1,n)\n",
        "    y = m*x+b+0.3*np.random.normal(0,1,n)\n",
        "    return (x,y)\n",
        "\n",
        "def simplest_training(n, k, eta):\n",
        "  np.random.seed(0)\n",
        "  # TODO: implement this method.\n",
        "  # Perform gradient descent k times with learning rate eta\n",
        "  # and n training data points.\n",
        "  # theta contains the learned parameters after training.\n",
        "\n",
        "  # initialize the variables and data\n",
        "  w = random.gauss(0,1)\n",
        "  b = 0\n",
        "  (x0, y0) = simplest_training_data(n)\n",
        "  for i in range(k):\n",
        "    (dw, db) = gradient_loss(w, x0, b, y0)\n",
        "    (w_new, b_new) = (w-eta*dw, b-eta*db)\n",
        "    (w, b) = (w_new, b_new)\n",
        "\n",
        "  theta = (w_new, b_new)\n",
        "  return theta \n",
        "\n",
        "\n",
        "def simplest_testing(theta, x):\n",
        "  # TODO: implement this method.\n",
        "  # Use the learned theta to classify given test data x.\n",
        "  # y stores the list of labels predicted by the model corresponding to each data in x.\n",
        "  y = []\n",
        "  (w, b) = theta\n",
        "\n",
        "  for xi in x:\n",
        "    y.append(w*xi+b)\n",
        "\n",
        "  return y\n",
        "\n",
        "def simplest_loss(theta, x):\n",
        "  # TODO: implement this method for finding average loss\n",
        "  # Compute the average loss (use the loss function given in the question)\n",
        "  # for data points in x using theta.\n",
        "  # Return loss, a scalar value.\n",
        "  n = len(x)\n",
        "  y = simplest_testing(theta, x)\n",
        "  loss_ret = []\n",
        "\n",
        "  for i in range(n):\n",
        "    L = (1+m.exp(-(3*x[i]+2-y[i]))**2)**(-1)-1/2\n",
        "    loss_ret.append(L)\n",
        "  \n",
        "  loss = mean(loss_ret)\n",
        "  return loss\n",
        "\n",
        "def deriv_scaler(w, xi, b, yi):\n",
        "  first = -(1+m.exp(-(w*xi+b-yi)**(2)))**(-2)\n",
        "  second = m.exp(-(w*xi+b-yi)**(2))\n",
        "  third = -2*(w*xi+b-yi)\n",
        "  return first*second*third\n",
        "\n",
        "def gradient_loss(w, x, b, y):\n",
        "  n = len(x)\n",
        "  w_ret = []\n",
        "  b_ret = []\n",
        "  for i in range(n):\n",
        "    dw = deriv_scaler(w,x[i],b,y[i])*x[i]\n",
        "    db = deriv_scaler(w,x[i],b,y[i])\n",
        "    w_ret.append(dw)\n",
        "    b_ret.append(db)\n",
        "  return (mean(w_ret), mean(b_ret))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YBfX_xYsJPO",
        "outputId": "fc9cd3ea-10d6-4f20-d10a-88e373143146"
      },
      "source": [
        "theta = simplest_training(30, 10000, 0.000002)\n",
        "print(theta)\n",
        "x = [0,1,2,3,4,5,6,7,8,9]\n",
        "print(simplest_loss(theta, x))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(-1.3454366328178722, 0.00011422862119231614)\n",
            "0.49820066741086577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBZDJthCOAOw"
      },
      "source": [
        "### Analysis (10 Points)\n",
        "Answer the following questions (a - e):\n",
        "\n",
        "a) What is the average loss for test data for $x = (0, 1, ..., 9)$ using model\n",
        "with $n = 30, k = 10000, \\eta = 0.02$? (Only write the float value below)\n",
        "\n",
        "b) What is the average loss for test data for $x = (0, 1, ..., 9)$ using model\n",
        "with $n = 2, k = 10000, \\eta = 0.02$? (Only write the float value below)\n",
        "\n",
        "c) What is the average loss for test data for $x = (0, 1, ..., 9)$ using model\n",
        "with $n = 30, k = 100, \\eta = 5$? (Only write the float value below)\n",
        "\n",
        "d) What is the average loss for test data for $x = (0, 1, ..., 9)$ using model\n",
        "with $n = 30, k = 10000, \\eta = 0.000002$? (Only write the float value below)\n",
        "\n",
        "e) Are the average loss values you get in each cases above (b-d) differ from that (a)? Why? **(Explain in less than 50 words. You will be penalized if the answer exceeds the word limit.)**\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "- !!! _YOUR RESPONSE HERE_ !!!\n",
        "\n",
        "**a) 0.22819781696473718**\n",
        "\n",
        "**b) 0.4980285114750154**\n",
        "\n",
        "**c) 0.25608297009678727**\n",
        "\n",
        "**d) 0.4982006215855819**\n",
        "\n",
        "**e) (a) vs. (b): more sample for training = less loss; (a) vs. (c): more iteration = less loss and $\\eta$ doesn't affect much here; (a) vs. (d): too little $\\eta$ leads to more loss, as it converge too slowly.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KBU60MFa84o"
      },
      "source": [
        "## Problem 2 (35 Points)\n",
        "You will now create a network that is a little more complicated. It still contains just an input and an output layer, with no hidden layers. But it now has a nonlinearity along with a cross-entropy loss, so that we can use it for classification.\n",
        "<br>\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1UkNx6-HghYRsjrXbqB6jN04VUsA-_RKp&sz=w1000\" alt=\"net2\" width=\"400px\"/>\n",
        "<br>\n",
        "<i>Figure 2: Network for Problem 2</i>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "The network has two inputs, $x_1$ and $x_2$.  These are connected with two weights to a single output unit.  If we let $z = w_1x_1 + w_2x_2 + b$, the output unit will have an activation of $a = \\sigma(z)$, where $\\sigma(z)$ represents the sigmoid function:\n",
        "\n",
        "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "We can interpret the output as giving the probability that the input belongs to class 1. If the probability is low, then the input probably belongs to class 0. Hint: the derivative of the sigmoid is given by:\n",
        "\n",
        "$$\\frac{d\\sigma}{dz} = \\sigma(z)(1-\\sigma(z))$$\n",
        "\n",
        "In training the network, you will use the cross-entropy loss. In this case, the cross entropy loss will be:\n",
        "\n",
        "$$L_{CE}(x,y) = -(y\\log{a} + (1-y)\\log{(1-a)})$$\n",
        "\n",
        "If $y = 1$, this is just the negative log of what the network predicts for the probability that the input belongs to class 1.  If $y = 0$, it is the negative log of the probability that the input belongs to class 0.\n",
        "We provide you with a routine to generate training data. This has the form:\n",
        "\n",
        "```single_layer_training_data(trainset)```  \n",
        "    \n",
        "which returns $X$ and $y$.\n",
        "This provides two different training sets.  When the input, trainset, is 1, the function produces a simple, linearly separable training set.  Half the points are near $(0,0)$ and half are near $(10,10)$.  $X$ is a matrix in which each row contains one of these points, so it is $n \\times 2$, where $n$ is the number of points.  $y$ is a vector of class labels, which have the value 1 for the points near $(0,0)$ and 0 for the points near $(10,10)$.\n",
        "\n",
        "When trainset is 2, we generate a different training set that is not linearly separable, but that corresponds to the Xor problem.  Points from class 1 are either near $(0,0)$ or $(10,10)$, while points in class 0 are near either $(10,0)$ or $(0,10)$.\n",
        "\n",
        "You will need to implement two routines.  \n",
        "The first is: \n",
        "\n",
        "```single_layer_training(k, eta, trainset)```  \n",
        "  \n",
        "As before, $k$ will indicate the number of iterations of gradient descent and eta gives the learning rate.  trainset indicates which training set to use, 1 or 2.  You will train the network using the same gradient descent approach as in the previous problem.  As before, we suggest that you initialize weights using random values chosen from a Gaussian distribution with zero mean, and that you initialize bias at 0.  \n",
        "\n",
        "You will also implement a test routine: \n",
        "\n",
        "```single_layer_testing(theta, X)```  \n",
        "  \n",
        "This takes in the network parameters and a matrix, $X$, of the form returned by single\\_layer\\_training\\_data.  It returns a vector of the output values the network computes.\n",
        "\n",
        "__Remember__: The `trainset` argument is the integer to be used to generate data with `single_layer_training_data(trainset)`, it is NOT the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sLnCceZtHFv"
      },
      "source": [
        "###Problem 2\n",
        "###Provided function to create training data\n",
        "def single_layer_training_data(trainset):\n",
        "    n = 10\n",
        "    if trainset == 1:\n",
        "    # Linearly separable\n",
        "        X = np.concatenate((np.random.normal((0,0),1,(n,2)), np.random.normal((10,10),1,(n,2))),axis=0)\n",
        "        y = np.concatenate((np.ones(n), np.zeros(n)),axis=0)\n",
        "\n",
        "    elif trainset == 2:\n",
        "        # Not Linearly Separable\n",
        "        X = np.concatenate((np.random.normal((0,0),1,(n,2)), np.random.normal((10,10),1,(n,2)), np.random.normal((10,0),1,(n,2)), np.random.normal((0,10),1,(n,2))),axis=0)\n",
        "        y = np.concatenate((np.ones(2*n), np.zeros(2*n)), axis=0)\n",
        "\n",
        "    else:\n",
        "        print (\"function single_layer_training_data undefined for input\", trainset)\n",
        "        sys.exit()\n",
        "    return (X,y)\n",
        "\n",
        "def single_layer_training(k, eta, trainset):\n",
        "  #TODO: Your Code Here\n",
        "  w = [random.gauss(0,1), random.gauss(0,1)]\n",
        "  b = 0\n",
        "  (X0, y0) = single_layer_training_data(trainset)\n",
        "\n",
        "  for i in range(k):\n",
        "    w_new, b_new = train_2(eta, X0, y0, w, b)\n",
        "    w, b = w_new, b_new\n",
        "  theta = (w, b)\n",
        "  return theta\n",
        "\n",
        "def single_layer_testing(theta, X):\n",
        "  #TODO: Your Code Here\n",
        "  (w, b) = theta\n",
        "  z = sigmoid(np.dot(X, w) + b)\n",
        "  return z\n",
        "\n",
        "def forward_2(w, b, xi):\n",
        "  z = sigmoid(np.dot(xi, w) + b)\n",
        "  return z\n",
        "\n",
        "def backprop_2(xi, yi, z, w, b, eta):\n",
        "  error = yi - z\n",
        "  delta = error * sigmoid(z)*(1-sigmoid(z)) * eta\n",
        "  w += np.dot(xi.T, delta)\n",
        "  b += error*sigmoid(z)*eta\n",
        "  return w, b\n",
        "\n",
        "def train_2(eta, X, y, w, b):\n",
        "  n = len(X)\n",
        "  for i in range(n):\n",
        "    z = forward_2(w, b, X[i])\n",
        "    w, b = backprop_2(X[i], y[i], z, w, b, eta)\n",
        "  return w, b\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty4jZ23sPvVA"
      },
      "source": [
        "### Analysis (10 Points)\n",
        "Do not use the data you used to train the network, call `single_layer_training_data` again to get fresh data for testing. Answer the following questions in this cell:\n",
        "\n",
        "a) What is the value of theta when k = 10000, $\\eta$=0.01, for trainset = 1? \n",
        "\n",
        "b) Plot a figure for datapoints and the linearly separator the algorithm calculated under the condition in a).\n",
        "\n",
        "c) What is the value of theta when k = 10000, $\\eta$=0.0001, for trainset = 2? \n",
        "\n",
        "d) Plot a figure for datapoints and the linearly separator the algorithm calculated under the condition in c).\n",
        "\n",
        "e) When trainset = 1, your network should assign a high probability of belonging to class 1 for points near $(0,0)$, and a low probability for points near $(10,10)$.  When trainset = 2, the data is not linearly separable, so you may find that your network has problems being able to separate. Use sample network outputs to describe what happens for the two trainsets. **(Explain in less than 50 words. You will be penalized if the answer exceeds the word limit.)**\n",
        "\n",
        "\n",
        "!!! _YOUR RESPONSE HERE_ !!!\n",
        "- the following output is in the form of ([w1, w2], b)\n",
        "- **a) (array([-0.73841042, -1.07817158]), 6.639204151436298)**\n",
        "- **c) (array([-0.00371366, -0.00709988]), 0.03758684049994358)**\n",
        "- **trainset = 1: w started from random gaussian with mean=0 and would stop updating as soon as the dataset is linear sepatated. So it would close to 0.**\n",
        "- **trainset = 2: $\\eta$ is too small and would jump back and forth because of the Xor.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhC-GfXfPiu3",
        "outputId": "0b01e3e4-e35d-4c50-f25f-ade73413c485"
      },
      "source": [
        "theta = single_layer_training(10000, 0.01, 1)\n",
        "print(theta)\n",
        "(X,y) = single_layer_training_data(1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([-0.5763758 , -1.11754333]), 6.360247147711676)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "4V6r-a93baGf",
        "outputId": "bdc67632-0a81-4e4d-e0c6-566c3386cda8"
      },
      "source": [
        "X1 = []\n",
        "X2 = [] \n",
        "for i in X:\n",
        "  X1.append(i[0])\n",
        "  X2.append(i[1])\n",
        "([w1, w2], b) = theta\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(X1, X2, '*')\n",
        "x = np.linspace(-2,12,100)\n",
        "y = (-w1/w2)*x-(b/w2)\n",
        "plt.plot(x, y)\n",
        "plt.title(\"Problem 2(b)\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Problem 2(b)')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1NQ72yggsaV",
        "outputId": "1c08a4e2-0963-404e-d2e9-bcee5a32da13"
      },
      "source": [
        "theta = single_layer_training(10000, 0.0001, 2)\n",
        "print(theta)\n",
        "(X,y) = single_layer_training_data(2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([-0.01116073,  0.01421513]), -0.01183028828882523)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "4-UN8VWjg52v",
        "outputId": "69dc6361-8b64-4bbb-cb47-074bf7d87d27"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "for i in X:\n",
        "  X1.append(i[0])\n",
        "  X2.append(i[1])\n",
        "([w1, w2], b) = theta\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(X1, X2, '*')\n",
        "x = np.linspace(-2,12,100)\n",
        "y = (-w1/w2)*x-(b/w2)\n",
        "plt.plot(x, y)\n",
        "plt.title(\"Problem 2(d)\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Problem 2(d)')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBqMD3voqug8"
      },
      "source": [
        "## Problem 3 (40 Points)\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1lZQ1CnQUDD-kiyL-FtDu0UTO1dmEy-Oq&sz=w1000\" alt=\"net3\" width=\"400px\"/>\n",
        "<br>\n",
        "<i>Figure 3: Network for Problem 3</i>\n",
        "</center>\n",
        "<br>\n",
        "<br>\n",
        "Now you will implement a multi-layer network that has a hidden layer.  To start with a relatively simple case, we will do this without any non-linearities. The network has two input units, $x_1$ and $x_2$.  These are connected to a single hidden unit.  We'll call the activation of this hidden unit $h$, so $h = w_{11}x_1 + w_{12}x_2 + b_{11}$.  This hidden unit is connected to two output units.  We'll call their activation $z_1$ and $z_2$, so we have:\n",
        "\n",
        "$$z_1 = w_{21}h + b_{21}~~~~~~~~~~~z_2 = w_{22}h + b_{22}$$\n",
        "\n",
        "To train this network, we use a loss function that says that we want the output to be close to the input.  So the loss function is:  \n",
        "  \n",
        "$$L(x_1, x_2) = (z_1 - x_1)^2 + (z_2 - x_2)^2$$\n",
        "    \n",
        "That is, the input is also acting as the label.  This kind of network is called an **auto-encoder**.  You may be wondering what the point of this is.  Because the hidden layer is smaller than the input and output layers, the network is forced to learn low-dimensional representation of the data.  In this case, the network learns to map the input points onto a line in the hidden layer, and then compute the 2D coordinates of the points on this line for the output layer.  This process is called Principal Component Analysis (PCA).\n",
        "\n",
        "We will provide a routine to generate training data:\n",
        "\n",
        "```pca_training_data(n, sigma)```  \n",
        "  \n",
        "The input parameter $n$ indicates the number of points in the training set.  As in the last problem, $X$ contains a $n \\times 2$ matrix in which each row contains the coordinates of a 2D point.  These points are generated to lie along the line $y = x + 1$.  Then Gaussian noise is added to the points, with zero mean and a standard deviation of sigma.\n",
        "\n",
        "Once again, you will implement training and testing routines.  \n",
        "  \n",
        "`pca_training(k, eta, n, sigma)`  \n",
        "  \n",
        "The input $k$ gives the number of iterations of gradient descent to use, while $eta$ gives the learning rate.  The input value $n$ indicates the number of points in the training set, while $sigma$ indicates the amount of noise added to these points.  Use these as parameters to pca\\_training\\_data.  The routine returns theta, a representation of all the weights and biases in the network.\n",
        "Also implement a test routine: \n",
        "\n",
        "```pca_test(theta, X)```  \n",
        "  \n",
        "$X$ will contain test data in the form returned by pca\\_training\\_data.  $Z$ provides the results the network produces given this input; $Z$ has the same format as $X$.  \n",
        "\n",
        "To test this, try training the network with $n = 10$ and $sigma = .1$.  Then test, using the input: `pca_test(theta, [[1,2], [4,5], [10, 3]])`.  \n",
        "  \n",
        "When I run my  code with this test I get: `[[0.9418, 2.0653], [3.9543, 5.0511], [6.1780, 7.2551]]`.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI3FF-JptKqI"
      },
      "source": [
        "###Problem 3\n",
        "###Provided function to create training data\n",
        "def pca_training_data(n, sigma):\n",
        "    m = 1\n",
        "    b = 1\n",
        "    x1 = np.random.uniform(0,10,n)\n",
        "    x2 = m*x1+b\n",
        "    X = np.c_[x1, x2]\n",
        "    X += np.random.normal(0, sigma, X.shape)\n",
        "    return X\n",
        "\n",
        "def pca_training(k, eta, n, sigma):\n",
        "    #TODO: Your Code Here\n",
        "    w1 = np.array([random.gauss(0,1), random.gauss(0,1)])\n",
        "    w2 = np.array([random.gauss(0,1), random.gauss(0,1)])\n",
        "    b = 0\n",
        "    b2 = np.array([0, 0])\n",
        "    X = pca_training_data(n, sigma)\n",
        "\n",
        "    for i in range(k):\n",
        "      w1_new, w2_new, b_new, b2_new = train_3(w1, w2, b, b2, X, eta)\n",
        "      w1, w2, b, b2 = w1_new, w2_new, b_new, b2_new\n",
        "    theta = (w1, w2, b, b2)\n",
        "    return theta\n",
        "\n",
        "def pca_test(theta, X):\n",
        "    #TODO: Your Code Here\n",
        "    (w1, w2, b, b2) = theta\n",
        "    [w11, w12] = w1\n",
        "    [w21, w22] = w2\n",
        "    [b21, b22] = b2\n",
        "    Z = []\n",
        "\n",
        "    for xi in X:\n",
        "      (x1, x2) = xi\n",
        "      h = w11*x1+w12*x2+b\n",
        "      z1 = w21*h+b21\n",
        "      z2 = w22*h+b22\n",
        "      Z.append([z1, z2])\n",
        "\n",
        "    return Z\n",
        "\n",
        "def hidden_3(w1, b, xi):\n",
        "  [w11, w12] = w1\n",
        "  [x1, x2] = xi\n",
        "  h = w11*x1+w12*x2+b\n",
        "  return h\n",
        "\n",
        "def forward_3(w1, w2, b, b2, xi):\n",
        "  [x1, x2] = xi\n",
        "  [w11, w12] = w1\n",
        "  [w21, w22] = w2\n",
        "  [b21, b22] = b2\n",
        "\n",
        "  hi = hidden_3(w1, b, xi)\n",
        "  z1 = w21*hi+b21\n",
        "  z2 = w22*hi+b22\n",
        "  return [z1, z2]\n",
        "\n",
        "def gradient_loss_3(xi, zi, w1, w2, b):\n",
        "  [x1, x2] = xi\n",
        "  [z1, z2] = zi\n",
        "  [w11, w12] = w1\n",
        "  [w21, w22] = w2\n",
        "\n",
        "  h = w11*x1+w12*x2+b\n",
        "  dw11 = 2*(z1-x1)*w21*x1 + 2*(z2-x2)*w22*x1\n",
        "  dw12 = 2*(z1-x1)*w21*x2 + 2*(z2-x2)*w22*x2\n",
        "  dw21 = 2*(z1-x1)*h\n",
        "  dw22 = 2*(z2-x2)*h\n",
        "  db = 2*(z1-x1)*w21 + 2*(z2-x2)*w22\n",
        "  db21 = 2*(z1-x1)\n",
        "  db22 = 2*(z2-x2)\n",
        "\n",
        "  return [dw11, dw12], [dw21, dw22], db, [db21, db22]\n",
        "\n",
        "def backprop_3(w1, w2, b, b2, xi, eta):\n",
        "  [x1, x2] = xi\n",
        "  [w11, w12] = w1\n",
        "  [w21, w22] = w2\n",
        "  [b21, b22] = b2\n",
        "\n",
        "  zi = forward_3(w1, w2, b, b2, xi)\n",
        "  [delta_w11, delta_w12], [delta_w21, delta_w22], delta_b, [delta_b21, delta_b22] = gradient_loss_3(xi, zi, w1, w2, b)\n",
        "  w11_new = w11 - eta*delta_w11\n",
        "  w12_new = w12 - eta*delta_w12\n",
        "  w21_new = w21 - eta*delta_w21\n",
        "  w22_new = w22 - eta*delta_w22\n",
        "  b_new = b - eta*delta_b\n",
        "  b21_new = b21 - eta*delta_b21\n",
        "  b22_new = b22 - eta*delta_b22\n",
        "  return [w11_new, w12_new], [w21_new, w22_new], b_new, [b21_new, b22_new]\n",
        "\n",
        "def train_3(w1, w2, b, b2, X, eta):\n",
        "  n = len(X)\n",
        "  for i in range(n):\n",
        "    zi = forward_3(w1, w2, b, b2, X[i])\n",
        "    w1, w2, b, b2 = backprop_3(w1, w2, b, b2, X[i], eta)\n",
        "  return w1, w2, b, b2\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8-oiDwBHzdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edbfbea8-3cb2-49ac-de15-8cf56fe7ff9b"
      },
      "source": [
        "theta1 = pca_training(300, 0.0001, 10, 0.1)\n",
        "# print(theta)\n",
        "X = [[1,2], [4,5], [10, 3]]\n",
        "print(pca_test(theta1, X))\n",
        "\n",
        "theta2 = pca_training(300, 0.0001, 10, 0)\n",
        "print(pca_test(theta2, X))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.0073575541421533, 2.2670105167061534], [4.541653527687778, 5.147452028236985], [0.9540519732345343, 1.0698397240111375]]\n",
            "[[1.2933832215863224, 1.795974371427559], [4.138164095647702, 4.904532422237686], [5.290603165978267, 6.163829327250173]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwXLKsE9QQjt"
      },
      "source": [
        "### Analysis (10 Points)\n",
        "It may take a little work to find good values for $k$ and $\\eta$.  Add a description of your experimental results inside this cell.  \n",
        "\n",
        "a) Can you explain why the network produces the point $(6.1780, 7.2551)$ with an input of $(10, 3)$? **(Explain in less than 30 words. You will be penalized if the answer exceeds the word limit.)**\n",
        "\n",
        "b) Do another test with $sigma = 0$ instead of $sigma = .1$.  Run your network with the same test data.  How have the results changed?  Can you explain this change? **(Explain in less than 30 words. You will be penalized if the answer exceeds the word limit.)**\n",
        "\n",
        "\\\\\n",
        "\n",
        "- !!! _YOUR RESPONSE HERE_ !!!\n",
        "\n",
        "**a) The training follow the order of input, which means it will fit better to (1,2), (4,5). And it's likely to care more on the difference and summation of the two inputs.**\n",
        "\n",
        "- Output for sigma= .1\n",
        "- [[2.050895889464059, 2.020945695728913], [4.490924819978479, 5.016365808773104], [4.868753747075665, 5.480194886110404]]\n",
        "- Output for sigma = 0\n",
        "- [[1.299227373982601, 1.7967637598367587], [4.125322719428179, 4.915310489123544], [6.752073080287472, 7.81388351841357]]\n",
        "\n",
        "**b) It changed, closer to the correct output. The standard deviation for random training data is less, leaving less freedom to the randomization, which means more concentrated to the input.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wep92fjrcgK"
      },
      "source": [
        "## Problem 4 (optional challenge problem, for extra credit, 20 points):\n",
        "Ok, now you are ready to create a complete, fully connected neural net with a hidden layer and non-linearities. You will use this network to solve the XOR problem, using the same training data as in Problem 2. Your network architecture should have the following components:\n",
        "- Two input units, with activations $x_1$ and $x_2$.  These are just the coordinates of 2D points.\n",
        "- A variable number of hidden units, H.  Write your code so that you can select the number of hidden units as a hyperparameter.  Let's call the activation of the $i$'th hidden unit, $a^1_i$.  Let's call the weights of these units $w^1_{ij}$.  This is the weight from input unit $j$ to hidden unit $i$.  \n",
        "- Use a RELU non-linearity for the hidden units.  So to determine the activation of a hidden unit we have: $z^1_i = w^1_{i1}x_1 + w^1_{i2}x_2 + b^1_i$, and $a^1_i = max(0, z^1_i)$.\n",
        "- There is then a single output unit.  Call its activation $a^2$.  We compute this as: $z^2 = \\left( \\sum_{i=1}^H w^2_{1i}a^1_i \\right) + b^2$, and $a^2 = \\sigma(z^2)$, where $\\sigma$ is the sigmoid nonlinearity.  This last part is just the same as in Problem 2.  And, like Problem 2, you can train your network using the cross-entropy loss.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1qfLXZJsDFIwTfIdHxEP6HnJBmUrPGZsT&sz=w1000\" alt=\"net3\" width=\"400px\"/>\n",
        "<br>\n",
        "<i>Figure 4: Network for Problem 4</i>\n",
        "</center>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Implement test and training functions with the templates:\n",
        "\n",
        "`nn_training(k, eta, trainset, H)`\n",
        "\n",
        "`nn_testing(theta, X)`\n",
        "\n",
        "The parameters to the training routine are similar to those in Problem 2, with $H$ indicating the number of hidden units.  The testing routine has the same form as in Problem 2.\n",
        "\n",
        "__Remember__: The `trainset` argument is the integer to be used to generate data with `single_layer_training_data(trainset)`, it is not the actual training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3U68nvJtM1_"
      },
      "source": [
        "# ###Problem 4: Challenge Problem\n",
        "# def nn_training(k, eta, trainset, H):\n",
        "#     #TODO: Your Code Here\n",
        "#     w1 = np.full((H, 2), random.gauss(0,1))\n",
        "#     w2 = np.full((H, 1), random.gauss(0,1))\n",
        "#     b1 = 0\n",
        "#     b2 = 0\n",
        "#     (X, y) = single_layer_training_data(trainset)\n",
        "\n",
        "#     for i in range(k):\n",
        "#       w1_new, w2_new, b1_new, b2_new = train_4(X, y, w1, w2, b1, b2, eta)\n",
        "#       w1, w2, b1, b2 = w1_new, w2_new, b1_new, b2_new\n",
        "#     theta = (w1, w2, b1, b2)\n",
        "#     return theta\n",
        "\n",
        "# def nn_testing(theta, X):\n",
        "#     #TODO: Your Code Here\n",
        "#     (w1, w2, b1, b2) = theta\n",
        "#     y = []\n",
        "#     for Xi in X:\n",
        "#       Z1 = np.dot(Xi, w1.T) + b1\n",
        "#       a1 = ReLU(Z1)\n",
        "#       Z2 = np.dot(a1.T, w2) + b2 \n",
        "#       a2 = Sigmoid(Z2)\n",
        "#       y.append(a2[0])\n",
        "#     return y\n",
        "\n",
        "\n",
        "# def forward_4(w1, w2, b1, b2, Xi):\n",
        "#   # [w1, w2] = w\n",
        "#   # [b1, b2] = b\n",
        "\n",
        "#   # print(\"w1: \", w1.shape)\n",
        "#   # print(\"w2: \", w2.shape)\n",
        "#   # print(\"b1: \", b1)\n",
        "#   # print(\"b2: \", b2)\n",
        "#   # print(\"Xi: \", Xi.shape)\n",
        "#   Z1 = np.dot(Xi, w1.T) + b1\n",
        "#   a1 = ReLU(Z1)\n",
        "#   # print(\"a1: \", a1)\n",
        "#   Z2 = np.dot(a1.T, w2) + b2 \n",
        "#   # print(\"Z2: \", Z2)\n",
        "#   a2 = Sigmoid(Z2)\n",
        "#   # print(\"a2: \", a2)\n",
        "#   return [a1, a2]\n",
        "\n",
        "# def backprop_4(Xi, yi, a, w1, w2, b1, b2, eta):\n",
        "#   [a1, a2] = a\n",
        "#   # print(\"w1: \", w1)\n",
        "#   # print(\"w2: \", w2)\n",
        "#   # print(\"b1: \", b1)\n",
        "#   # print(\"b2: \", b2)\n",
        "#   # aaaaa = np.array([a1])\n",
        "#   # print(\"a1: \", aaaaa.T)\n",
        "#   # [w1, w2] = w\n",
        "#   # [b1, b2] = b\n",
        "\n",
        "#   error = yi - a2 \n",
        "#   # print(\"error: \", error)\n",
        "#   delta2 = error * Sigmoid(a2)*(1-Sigmoid(a2)) * eta\n",
        "#   # print(\"delta2: \", delta2)\n",
        "#   w2 += np.dot(np.array([a1]).T, np.array([delta2]))\n",
        "#   # print(\"w2: \", w2)\n",
        "#   b2 += error * Sigmoid(a2) * eta\n",
        "#   # print(\"b2: \", b2)\n",
        "\n",
        "#   for i in range(len(a1)):\n",
        "#     # print(\"a: \", a1[i])\n",
        "#     if a1[i] != 0:\n",
        "#       # print(\"w1: \", w1[i])\n",
        "#       # print(\"Xi: \", Xi)\n",
        "#       # print(np.dot(np.array([Xi]).T, np.array([delta2])).T)\n",
        "#       delta1 = np.dot(np.array([Xi]).T, np.array([delta2])).T\n",
        "#       w1[i] += delta1[0]\n",
        "#       b1 += error * Sigmoid(a2) * eta\n",
        "\n",
        "#   return w1, w2, b1, b2\n",
        "\n",
        "# def train_4(X, y, w1, w2, b1, b2, eta):\n",
        "#   # [w1, w2] = w\n",
        "#   # [b1, b2] = b\n",
        "\n",
        "#   n = len(X)\n",
        "#   for i in range(n):\n",
        "#     a = forward_4(w1, w2, b1, b2, X[i])\n",
        "#     w1, w2, b1, b2 = backprop_4(X[i], y[i], a, w1, w2, b1, b2, eta)\n",
        "#   return w1, w2, b1, b2\n",
        "\n",
        "# def Sigmoid(z):\n",
        "#   return 1/(1+np.exp(-z))\n",
        "\n",
        "# def ReLU(Z):\n",
        "#   a = []\n",
        "#   for z in Z:\n",
        "#     if z > 0:\n",
        "#       a.append(z)\n",
        "#     else:\n",
        "#       a.append(0)\n",
        "#   return np.array(a)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYv_kOuH0ds7"
      },
      "source": [
        "# theta = nn_training(1000, 0.1, 2, 4)\n",
        "# # print(theta)\n",
        "# (X, y) = single_layer_training_data(2)\n",
        "# print(\"X: \", X)\n",
        "# print(\"y: \", y)\n",
        "# y = nn_testing(theta, X)\n",
        "# print(y)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taomdQsok5Bb"
      },
      "source": [
        "# X1 = []\n",
        "# X2 = [] \n",
        "# for i in X:\n",
        "#   X1.append(i[0])\n",
        "#   X2.append(i[1])\n",
        "# (w1, w2, b1, b2) = theta\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(X1, X2, '*')\n",
        "# x = np.linspace(-2,12,100)\n",
        "# y = nn_testing(theta, X)\n",
        "# plt.plot(x, y)\n",
        "# plt.title(\"Problem 2(b)\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U99gXCuoWtd"
      },
      "source": [
        "# class MLP:\n",
        "#     \"\"\"\n",
        "#     Create a multi-layer perceptron.\n",
        "\n",
        "#     train_data: A 4x2 matrix with the input data.\n",
        "\n",
        "#     target: A 4x1 matrix with expected outputs\n",
        "\n",
        "#     lr: the learning rate. Defaults to 0.1\n",
        "\n",
        "#     num_epochs: the number of times the training data goes through the model\n",
        "#         while training\n",
        "\n",
        "#     num_input: the number of nodes in the input layer of the MLP.\n",
        "#         Should be equal to the second dimension of train_data.\n",
        "    \n",
        "#     num_hidden: the number of nodes in the hidden layer of the MLP.\n",
        "\n",
        "#     num_output: the number of nodes in the output layer of the MLP.\n",
        "#         Should be equal to the second dimension of target.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, train_data, target, lr=0.1, num_epochs=100, num_input=2, num_hidden=2, num_output=1):\n",
        "#         self.train_data = train_data\n",
        "#         self.target = target\n",
        "#         self.lr = lr\n",
        "#         self.num_epochs = num_epochs\n",
        "\n",
        "#         # initialize both sets of weights and biases randomly\n",
        "#             # - weights_01: weights between input and hidden layer\n",
        "#             # - weights_12: weights between hidden and output layer\n",
        "#         self.weights_01 = np.random.uniform(size=(num_input, num_hidden))\n",
        "#         self.weights_12 = np.random.uniform(size=(num_hidden, num_output))\n",
        "\n",
        "#         # - b01: biases for the  hidden layer\n",
        "#         # - b12: bias for the output layer\n",
        "#         self.b01 = np.random.uniform(size=(1,num_hidden))\n",
        "#         self.b12 = np.random.uniform(size=(1,num_output))\n",
        "\n",
        "#         self.losses = []\n",
        "\n",
        "#     def update_weights(self):\n",
        "        \n",
        "#         # Calculate the squared error\n",
        "#         loss = 0.5 * (self.target - self.output_final) ** 2\n",
        "#         print(loss)\n",
        "#         self.losses.append(np.sum(loss))\n",
        "\n",
        "#         error_term = (self.target - self.output_final)\n",
        "\n",
        "#         # the gradient for the hidden layer weights\n",
        "#         grad01 = self.train_data.T @ (((error_term * self._delsigmoid(self.output_final)) * self.weights_12.T) * self._delsigmoid(self.hidden_out))\n",
        "#         print(\"grad01: \", grad01)\n",
        "#         print(grad01.shape)\n",
        "\n",
        "#         # the gradient for the output layer weights\n",
        "#         grad12 = self.hidden_out.T @ (error_term * self._delsigmoid(self.output_final))\n",
        "\n",
        "#         print(\"grad12: \", grad12)\n",
        "#         print(grad12.shape)\n",
        "\n",
        "#         # updating the weights by the learning rate times their gradient\n",
        "#         self.weights_01 += self.lr * grad01\n",
        "#         self.weights_12 += self.lr * grad12\n",
        "\n",
        "#         # update the biases the same way\n",
        "#         self.b01 += np.sum(self.lr * ((error_term * self._delsigmoid(self.output_final)) * self.weights_12.T) * self._delsigmoid(self.hidden_out), axis=0)\n",
        "#         self.b12 += np.sum(self.lr * error_term * self._delsigmoid(self.output_final), axis=0)\n",
        "\n",
        "#     def _sigmoid(self, x):\n",
        "#         \"\"\"\n",
        "#         The sigmoid activation function.\n",
        "#         \"\"\"\n",
        "#         return 1 / (1 + np.exp(-x))\n",
        "\n",
        "#     def _delsigmoid(self, x):\n",
        "#         \"\"\"\n",
        "#         The first derivative of the sigmoid function wrt x\n",
        "#         \"\"\"\n",
        "#         return x * (1 - x)\n",
        "\n",
        "#     def forward(self, batch):\n",
        "#         \"\"\"\n",
        "#         A single forward pass through the network.\n",
        "#         Implementation of wX + b\n",
        "#         \"\"\"\n",
        "\n",
        "#         self.hidden_ = np.dot(batch, self.weights_01) + self.b01\n",
        "#         self.hidden_out = self._sigmoid(self.hidden_)\n",
        "\n",
        "#         self.output_ = np.dot(self.hidden_out, self.weights_12) + self.b12\n",
        "#         self.output_final = self._sigmoid(self.output_)\n",
        "\n",
        "#         return self.output_final\n",
        "\n",
        "#     def classify(self, datapoint):\n",
        "#         \"\"\"\n",
        "#         Return the class to which a datapoint belongs based on\n",
        "#         the perceptron's output for that point.\n",
        "#         \"\"\"\n",
        "#         datapoint = np.transpose(datapoint)\n",
        "#         if self.forward(datapoint) >= 0.5:\n",
        "#             return 1\n",
        "\n",
        "#         return 0\n",
        "\n",
        "#     def plot(self, h=0.01):\n",
        "#         \"\"\"\n",
        "#         Generate plot of input data and decision boundary.\n",
        "#         \"\"\"\n",
        "#         # setting plot properties like size, theme and axis limits\n",
        "#         sns.set_style('darkgrid')\n",
        "#         plt.figure(figsize=(20, 20))\n",
        "\n",
        "#         plt.axis('scaled')\n",
        "#         plt.xlim(-0.1, 1.1)\n",
        "#         plt.ylim(-0.1, 1.1)\n",
        "\n",
        "#         colors = {\n",
        "#             0: \"ro\",\n",
        "#             1: \"go\"\n",
        "#         }\n",
        "\n",
        "#         # plotting the four datapoints\n",
        "#         for i in range(len(self.train_data)):\n",
        "#             plt.plot([self.train_data[i][0]],\n",
        "#                      [self.train_data[i][1]],\n",
        "#                      colors[self.target[i][0]],\n",
        "#                      markersize=20)\n",
        "\n",
        "#         x_range = np.arange(-0.1, 1.1, h)\n",
        "#         y_range = np.arange(-0.1, 1.1, h)\n",
        "\n",
        "#         # creating a mesh to plot decision boundary\n",
        "#         xx, yy = np.meshgrid(x_range, y_range, indexing='ij')\n",
        "#         Z = np.array([[self.classify([x, y]) for x in x_range] for y in y_range])\n",
        "\n",
        "#         # using the contourf function to create the plot\n",
        "#         plt.contourf(xx, yy, Z, colors=['red', 'green', 'green', 'blue'], alpha=0.4)\n",
        "\n",
        "#     def train(self):\n",
        "#         \"\"\"\n",
        "#         Train an MLP. Runs through the data num_epochs number of times.\n",
        "#         A forward pass is done first, followed by a backward pass (backpropagation)\n",
        "#         where the networks parameter's are updated.\n",
        "#         \"\"\"\n",
        "#         for _ in range(self.num_epochs):\n",
        "#             self.forward(self.train_data)\n",
        "#             self.update_weights()\n",
        "            "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AZjpEZmo9iD"
      },
      "source": [
        "###Problem 4: Challenge Problem\n",
        "def nn_training(k, eta, trainset, H):\n",
        "    #TODO: Your Code Here\n",
        "    (X, y) = single_layer_training_data(trainset)\n",
        "    w1 = np.random.uniform(size=(2, H))\n",
        "    w2 = np.random.uniform(size=(H, 1))\n",
        "    b1 = np.random.uniform(size=(1,H))\n",
        "    b2 = np.random.uniform(size=(1,1))\n",
        "\n",
        "    for n in range(k):\n",
        "      for i in range(len(X)):\n",
        "        a1, a2 = forward_4(X[i], w1, w2, b1, b2)\n",
        "        w1, w2, b1, b2 = backprop_4(X[i], y[i], a1, a2, w1, w2, b1, b2, eta)\n",
        "    \n",
        "    theta = (w1, w2, b1, b2)\n",
        "    return theta\n",
        "\n",
        "def nn_testing(theta, X):\n",
        "    #TODO: Your Code Here\n",
        "    (w1, w2, b1, b2) = theta\n",
        "    y = []\n",
        "    for i in range(len(X)):\n",
        "      Z1 = np.dot(X[i], w1) + b1\n",
        "      a1 = ReLU(Z1)\n",
        "      Z2 = np.dot(a1, w2) + b2\n",
        "      a2 = Sigmoid(Z2)\n",
        "      y.append(a2[0][0])\n",
        "    return y\n",
        "\n",
        "def Sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "def Sigmoid_deriv(z):\n",
        "  return z*(1-z)\n",
        "\n",
        "def ReLU(Z):\n",
        "  a = []\n",
        "  for z in Z[0]:\n",
        "    if z > 0:\n",
        "      a.append(z)\n",
        "    else:\n",
        "      a.append(0)\n",
        "  return np.array([a])\n",
        "\n",
        "def forward_4(Xi, w1, w2, b1, b2):\n",
        "  Z1 = np.dot(Xi, w1) + b1\n",
        "  # Z1 = Z1[0]\n",
        "  # print(Z1)\n",
        "  a1 = ReLU(Z1)\n",
        "  # print(a1)\n",
        "  Z2 = np.dot(a1, w2) + b2\n",
        "  # Z2 = Z2[0]\n",
        "  # print(Z2) \n",
        "  a2 = Sigmoid(Z2)\n",
        "  # print(a2)\n",
        "  return a1, a2\n",
        "\n",
        "# def classify(data, theta):\n",
        "#   data = np.transpose(data)\n",
        "#   (w1, w2, b1, b2) = theta\n",
        "#   a1, a2 = forward_4(data, w1, w2, b1, b2)\n",
        "#   if a2 >= 0.5:\n",
        "#     return 1\n",
        "#   return 0\n",
        "\n",
        "def backprop_4(Xi, yi, a1, a2, w1, w2, b1, b2, eta):\n",
        "  error = yi - a2\n",
        "  # print(a1)\n",
        "  # print(\"w1: \", w1)\n",
        "  # print(\"b1: \", b1)\n",
        "  # print(\"w2: \", w2)\n",
        "  # print(\"b2: \", b2)\n",
        "  # print(a1.T)\n",
        "  Xi = np.array([Xi])\n",
        "  # print(Xi)\n",
        "  delta2 = a1.T @ (error * Sigmoid_deriv(a2))\n",
        "  # print(\"delta2: \", delta2)\n",
        "  delta1 = Xi.T @ (error * Sigmoid_deriv(a2))\n",
        "  # print(\"delta1: \", delta1)\n",
        "\n",
        "  w2 += eta * delta2\n",
        "  b2 += np.sum(eta * error * Sigmoid_deriv(a2), axis=0)\n",
        "  w1 += eta * delta1\n",
        "  b1 += np.sum(eta * error * Sigmoid_deriv(a2), axis=0)\n",
        "  # for i in range(len(a1[0])):\n",
        "  #   if a1[0][i] != 0:\n",
        "  #     w1[i] += eta * delta1\n",
        "  #     b1[i] += np.sum(eta * error * Sigmoid_deriv(a2), axis=0)\n",
        "\n",
        "  return w1, w2, b1, b2\n",
        "\n",
        "# import seaborn as sns\n",
        "# def plot(X, y, theta):\n",
        "#   sns.set_style('darkgrid')\n",
        "#   plt.figure(figsize=(20, 20))\n",
        "#   plt.axis('scaled')\n",
        "#   plt.xlim(-0.1, 1.1)\n",
        "#   plt.ylim(-0.1, 1.1)\n",
        "\n",
        "#   colors = {\n",
        "#       0: \"ro\",\n",
        "#       1: \"go\"\n",
        "#   }\n",
        "\n",
        "#   for i in range(len(X)):\n",
        "#     plt.plot(X[i][0], X[i][1], colors[y[i]], markersize=20)\n",
        "\n",
        "#   x_range = np.arange(-0.1, 1.1, 0.01)\n",
        "#   y_range = np.arange(-0.1, 1.1, 0.01)\n",
        "#   xx, yy = np.meshgrid(x_range, y_range, indexing = 'ij')\n",
        "#   Z = np.array([[classify([x, y], theta) for x in x_range] for y in y_range])\n",
        "#   plt.contour(xx, yy, Z, colors=['red', 'green', 'green', 'blue'], alpha=0.4)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq-EMW9zxn5M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CdYZTfL4xfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a340d6d-b30b-479e-d145-7d9f414bbb1b"
      },
      "source": [
        "theta = nn_training(100, 0.1, 2, 4)\n",
        "print(theta)\n",
        "(X, y) = single_layer_training_data(2)\n",
        "y = nn_testing(theta, X)\n",
        "print(y)\n",
        "# plot(X,y, theta)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([[ 46.88509994,  46.58661916,  46.65662038,  46.43618602],\n",
            "       [-49.95696966, -50.41249108, -49.66203798, -49.77806597]]), array([[1.35313515],\n",
            "       [0.59476766],\n",
            "       [0.23298027],\n",
            "       [0.06212127]]), array([[ 0.51560394,  0.33227936, -0.28228121,  0.10408065]]), array([[-0.03483761]]))\n",
            "[0.9676763001633969, 0.491291477386099, 0.9999998728340602, 1.0, 0.491291477386099, 0.491291477386099, 1.0, 0.491291477386099, 0.491291477386099, 1.0, 0.491291477386099, 0.491291477386099, 1.0, 0.491291477386099, 0.491291477386099, 0.491291477386099, 0.491291477386099, 0.491291477386099, 0.491291477386099, 0.9999999999991556, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.491291477386099, 0.491291477386099, 0.491291477386099, 0.491291477386099, 0.491291477386099, 0.491291477386099, 0.491291477386099, 0.491291477386099, 0.491291477386099, 0.491291477386099]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiwDxqDnup53"
      },
      "source": [
        "# ###Problem 4: Challenge Problem\n",
        "# def nn_training(k, eta, trainset, H):\n",
        "#     #TODO: Your Code Here\n",
        "#     return theta\n",
        "\n",
        "# def nn_testing(theta, X):\n",
        "#     #TODO: Your Code Here\n",
        "#     return y"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJcDH94VQ3iV"
      },
      "source": [
        "### Analysis (5 Points)\n",
        "Run experiments to demonstrate that your network can solve the XOR problem. How do you find the results vary as you vary the number of hidden units?  Show and discuss the results of your experiments inside/below this cell.\n",
        "\n",
        "- !!! _YOUR RESPONSE HERE_ !!!"
      ]
    }
  ]
}